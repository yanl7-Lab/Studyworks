{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037316f6-d705-4303-8966-e2a45ee3324b",
   "metadata": {},
   "source": [
    "# 第一章 1.3 Jieba分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22347793-6ef0-4821-8b2c-bb12ac9fe7c6",
   "metadata": {},
   "source": [
    "## 1.3.1 基于N-gram模型的中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5cdc19-8d79-4add-84d2-c9cc2cbe7947",
   "metadata": {},
   "source": [
    "### N-gram模型原理及其在分词中的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e6c26-1752-48e1-820e-ece96bf79ab7",
   "metadata": {},
   "source": [
    "**N-gram模型原理**  \n",
    "N-gram是一种基于统计的语言模型，假设当前词的出现概率仅依赖于前N-1个词。数学表达为：\\[ P(w_i|w_1,w_2,...,w_{i-1}) \\approx P(w_i|w_{i-n+1},...,w_{i-1}) \\]\n",
    "\n",
    "**在分词中的应用**  \n",
    "1. 二元模型（Bigram）最常用，计算相邻词的共现概率：\\[ P(W) = \\prod_{i=2}^n P(w_i|w_{i-1}) \\]  \n",
    "2. 通过统计语料库中词序列频率，选择概率最大的分词组合。例如：\"研究生命\"可切分为\"研究/生命\"（P=0.8）或\"研究生/命\"（P=0.2），模型会选择前者。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4556f0e-e257-44a4-8685-fd99907bd194",
   "metadata": {},
   "source": [
    "### 1.3.2基于隐马尔可夫模型（HMM）的中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17021707-25f0-4f51-a333-0c9be80428a2",
   "metadata": {},
   "source": [
    "**HMM建模过程**  \n",
    "定义五元组 \\( \\lambda = (S, O, A, B, \\pi) \\)：状态集S：{B（词首）, M（词中）, E（词尾）, S（单字词）} ，观测序列O：待分词的字符序列，状态转移矩阵A：B→M→E的概率，发射矩阵B：状态生成特定字符的概率（如\"的\"在S状态概率高），初始概率π：首字符各状态的分布\n",
    "\n",
    "**分词实现方法**  \n",
    " Viterbi算法求解最优状态序列  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51be641-f528-4411-a06a-1cd1985f5f6f",
   "metadata": {},
   "source": [
    "### 1.3.3 Jieba分词原理与流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ea659-0da4-411e-be4a-359b351df2d3",
   "metadata": {},
   "source": [
    "**核心算法**  \n",
    " *前缀词典*：加载预定义的词频词典（如\"北京大学\"：频次=1000），构建Trie树实现高效前缀匹配，时间复杂度O(k)（k为词长）\n",
    " *动态规划*（DP）：对未登录词，用DP求最大概率路径：\\[ \\max \\prod_{i=1}^n P(w_i) \\]，其中 \\( P(w_i) \\) 通过语料统计获得（如\"中国\"的log概率=-8.3）\n",
    "\n",
    "**分词流程**  \n",
    " *初始化*：加载词典（35万条词，默认文件`dict.txt`），*切分阶段*：用Trie树匹配所有可能成词，对歧义句（如\"结婚的和尚未结婚的\"）生成DAG图，*消歧阶段*：对DAG执行Viterbi算法（HMM模型处理未登录词），联合词典概率和HMM结果选择最优切分，*结果调整*：处理特殊规则（如英文、数字合并）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22cff50c-a1ed-4ea2-b620-2ea427d80286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_max_matching(vocab, sentence):\n",
    "    \"\"\"\n",
    "    基于正向最大匹配算法 (FMM) 的中文分词函数\n",
    "\n",
    "    参数：\n",
    "    vocab (list): 词典，包含了所有可能的词汇\n",
    "    sentence (str): 需要分词的句子\n",
    "\n",
    "    返回：\n",
    "    list: 分词结果的列表\n",
    "    \"\"\"\n",
    "    fmmresult = []  # 存储分词结果的列表\n",
    "    max_len = max([len(item) for item in vocab])  # 获取词典中最长词的长度\n",
    "    start = 0  # 分词的起始位置\n",
    "\n",
    "    # 开始遍历句子，直到处理完整个句子\n",
    "    while start != len(sentence):\n",
    "        index = start + max_len  # 尝试匹配最大长度的词\n",
    "        if index > len(sentence):  # 如果索引超出句子长度，修正为句子末尾\n",
    "            index = len(sentence)\n",
    "\n",
    "        # 从当前起始位置尝试从最大长度开始逐步缩小词的长度进行匹配\n",
    "        while index > start:\n",
    "            current_substr = sentence[start:index]  # 截取当前子串\n",
    "            # 如果子串在词典中，或者子串长度为1，则认为是一个有效词\n",
    "            if current_substr in vocab or len(current_substr) == 1:\n",
    "                fmmresult.append(current_substr)  # 将有效词加入结果列表\n",
    "                start = index  # 更新起始位置，跳过已处理的部分\n",
    "                break  # 找到一个有效词后跳出内层循环继续处理下一个子串\n",
    "            index -= 1  # 如果没有匹配到有效词，缩短子串长度再试\n",
    "\n",
    "    return fmmresult  # 返回最终的分词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae8719ad-9cba-4de5-9ed3-187617afdb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_directional_max_matching(vocab, sentence):\n",
    "    \"\"\"\n",
    "    Reverse Maximum Matching (RMM) 分词算法。\n",
    "    从句子的末尾开始，尝试从词典中匹配最长的词直到句子被分割。\n",
    "\n",
    "    Args:\n",
    "    vocab (list): 词典，包含了所有可能的词汇\n",
    "    sentence (str): 需要分词的句子\n",
    "\n",
    "    返回：\n",
    "    list: 分词后的结果，按顺序返回分词列表\n",
    "    \"\"\"\n",
    "    rmmresult = []  # 存储分词结果\n",
    "    max_len = max([len(item) for item in vocab])  # 获取词典中最大词的长度\n",
    "    start = len(sentence)  # 从句子的末尾开始\n",
    "\n",
    "    while start != 0:  # 直到处理完整个句子\n",
    "        index = start - max_len  # 尝试从当前位置往前推最大长度的子串\n",
    "        if index < 0:\n",
    "            index = 0  # 防止下标越界，调整为从0开始\n",
    "\n",
    "        while index < start:  # 向前查找直到找到匹配的词\n",
    "            current_substr = sentence[index:start]  # 截取当前子串\n",
    "\n",
    "            # 如果当前子串在词典中，或当前子串长度为1（即单个字符）\n",
    "            if current_substr in vocab or len(current_substr) == 1:\n",
    "                rmmresult.insert(0, current_substr)  # 匹配成功，插入到结果列表的开头\n",
    "                start = index  # 更新起始位置，继续向前匹配\n",
    "                break  # 找到一个词后跳出内层循环\n",
    "            index += 1  # 如果当前子串没有匹配，向前移动一个字符继续尝试\n",
    "\n",
    "    return rmmresult  # 返回最终的分词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02af4610-27fc-4f8f-a533-af22ca24ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_directional_matching(vocab, sentence):\n",
    "    # 获取正向和反向最大匹配的分词结果\n",
    "    res1 = forward_max_matching(vocab, sentence)\n",
    "    res2 = reverse_directional_max_matching(vocab, sentence)\n",
    "\n",
    "    len_res1, len_res2 = len(res1), len(res2)  # 保存长度\n",
    "\n",
    "    # 如果两个结果的长度相同\n",
    "    if len_res1 == len_res2:\n",
    "        # 如果两个结果相同，直接返回\n",
    "        if res1 == res2:\n",
    "            return res1\n",
    "        else:\n",
    "            # 统计每个结果中长度为1的词的数量\n",
    "            res1_sn = sum(1 for i in res1 if len(i) == 1)\n",
    "            res2_sn = sum(1 for i in res2 if len(i) == 1)\n",
    "            # 返回包含较少单字符词的结果\n",
    "            return res1 if res1_sn < res2_sn else res2\n",
    "    else:\n",
    "        # 返回词数较少的结果\n",
    "        return res1 if len_res1 < len_res2 else res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4211d22-2aa0-49d2-a5c5-fba21c72cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['我们', '今天', '在', '野生动物园', '玩']\n",
    "sentence = '我们是今天在野生动物园玩了'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbf81223-6e47-43bf-8345-01c7261a6a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我们', '是', '今天', '在', '野生动物园', '玩', '了']\n",
      "['我们', '是', '今天', '在', '野生动物园', '玩', '了']\n"
     ]
    }
   ],
   "source": [
    "fmm_result = forward_max_matching(vocab, sentence)\n",
    "rmm_result = reverse_directional_max_matching(vocab, sentence)\n",
    "print(fmm_result)\n",
    "print(rmm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8c83988-f7b9-41f8-a2d7-ae8ba950786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我们', '是', '今天', '在', '野生动物园', '玩', '了']\n"
     ]
    }
   ],
   "source": [
    "bm_result = bi_directional_matching(vocab, sentence)\n",
    "print(bm_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
